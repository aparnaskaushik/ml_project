{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QYGmZFsDSZo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbb0542-2343-43ce-9c93-ffd2e215f160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "{'1000268201_693b08cb0e': ['A little girl in a pink dress going into a wooden cabin .'], '1001773457_577c3a7d70': ['Two dogs on pavement moving toward each other .'], '1002674143_1b742ab4b8': ['Young girl with pigtails painting outside in the grass .'], '1003163366_44323f5815': ['man laying on bench holding leash of dog sitting on ground'], '1007129816_e794419615': ['The man with pierced ears is wearing glasses and an orange hat .'], '1007320043_627395c3d8': ['The small child climbs on a red ropes on a playground .'], '1009434119_febe49276a': ['A dog runs on the green grass near a wooden fence .'], '1012212859_01547e3f17': ['White dog with brown ears standing near water with head turned to one side .'], '1015118661_980735411b': ['Smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him .'], '1015584366_dfcec3c85a': ['The black dog jumped the tree stump .'], '101654506_8eb26cfb60': ['The white and brown dog is running over the surface of the snow .'], '101669240_b2d3e7f17b': ['Man on skis looking at artwork for sale in the snow'], '1016887272_03199f49c4': ['Several climbers in a row are climbing the rock while the man in red watches and holds the line .'], '1019077836_6fc9b15408': ['Large brown dog running away from the sprinkler in the grass .'], '1019604187_d087bf9a5f': ['A white dog running after a yellow ball'], '1020651753_06077ec457': ['The white dog is playing in a green field with a yellow toy .'], '1022454332_6af2c1449a': ['\"Two people are at the edge of a lake , facing the water and the city skyline .\"'], '1022454428_b6b660a67b': ['Couple with a baby sit outdoors next to their stroller .'], '1022975728_75515238d8': ['This is a black dog splashing in the water .'], '102351840_323e3de834': ['Two men are ice fishing .']}\n",
            "['1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1000268201_693b08cb0e', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1001773457_577c3a7d70', '1002674143_1b742ab4b8', '1002674143_1b742ab4b8', '1002674143_1b742ab4b8', '1002674143_1b742ab4b8', '1002674143_1b742ab4b8', '1003163366_44323f5815', '1003163366_44323f5815', '1003163366_44323f5815', '1003163366_44323f5815', '1003163366_44323f5815', '1007129816_e794419615', '1007129816_e794419615', '1007129816_e794419615', '1007129816_e794419615', '1007129816_e794419615', '1007320043_627395c3d8', '1007320043_627395c3d8', '1007320043_627395c3d8', '1007320043_627395c3d8', '1007320043_627395c3d8', '1009434119_febe49276a', '1009434119_febe49276a', '1009434119_febe49276a', '1009434119_febe49276a', '1009434119_febe49276a', '1012212859_01547e3f17', '1012212859_01547e3f17', '1012212859_01547e3f17', '1012212859_01547e3f17', '1012212859_01547e3f17', '1015118661_980735411b', '1015118661_980735411b', '1015118661_980735411b', '1015118661_980735411b', '1015118661_980735411b', '1015584366_dfcec3c85a', '1015584366_dfcec3c85a', '1015584366_dfcec3c85a', '1015584366_dfcec3c85a', '1015584366_dfcec3c85a', '101654506_8eb26cfb60', '101654506_8eb26cfb60', '101654506_8eb26cfb60', '101654506_8eb26cfb60', '101654506_8eb26cfb60', '101669240_b2d3e7f17b', '101669240_b2d3e7f17b', '101669240_b2d3e7f17b', '101669240_b2d3e7f17b', '101669240_b2d3e7f17b', '1016887272_03199f49c4', '1016887272_03199f49c4', '1016887272_03199f49c4', '1016887272_03199f49c4', '1016887272_03199f49c4', '1019077836_6fc9b15408', '1019077836_6fc9b15408', '1019077836_6fc9b15408', '1019077836_6fc9b15408', '1019077836_6fc9b15408', '1019604187_d087bf9a5f', '1019604187_d087bf9a5f', '1019604187_d087bf9a5f', '1019604187_d087bf9a5f', '1019604187_d087bf9a5f', '1020651753_06077ec457', '1020651753_06077ec457', '1020651753_06077ec457', '1020651753_06077ec457', '1020651753_06077ec457', '1022454332_6af2c1449a', '1022454332_6af2c1449a', '1022454332_6af2c1449a', '1022454332_6af2c1449a', '1022454332_6af2c1449a', '1022454428_b6b660a67b', '1022454428_b6b660a67b', '1022454428_b6b660a67b', '1022454428_b6b660a67b', '1022454428_b6b660a67b', '1022975728_75515238d8', '1022975728_75515238d8', '1022975728_75515238d8', '1022975728_75515238d8', '1022975728_75515238d8', '102351840_323e3de834', '102351840_323e3de834', '102351840_323e3de834', '102351840_323e3de834', '102351840_323e3de834']\n"
          ]
        }
      ],
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "#Load the dataset\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "directory_path = \"/content/drive/My Drive/image_captions\"\n",
        "\n",
        "caption_path = os.path.join(directory_path, 'captions.txt')\n",
        "caption_dict = {}\n",
        "image_file_names = []\n",
        "with open(caption_path, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  # because first line only contains heading image,caption\n",
        "  # taking only 100 pics as whole set\n",
        "  for line in lines[1:101]:\n",
        "    line = line.strip()\n",
        "    image_path, caption = line.split(',', maxsplit=1)\n",
        "    caption_dict[image_path.split('.')[0]] = caption.split(';')\n",
        "    image_file_names.append(image_path.split('.')[0])\n",
        "\n",
        "print(caption_dict)\n",
        "print(image_file_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "indices = list(range(len(image_file_names)))\n",
        "random.shuffle(indices)\n",
        "train_index = int(0.6*len(image_file_names))\n",
        "validation_index = int(0.8*len(image_file_names))\n",
        "\n",
        "# Spitting data\n",
        "training_file_names = [image_file_names[i] for i in indices[: train_index]]\n",
        "validation_file_names = [image_file_names[i] for i in indices[train_index : validation_index]]\n",
        "testing_file_names = [image_file_names[i] for i in indices[validation_index:]]"
      ],
      "metadata": {
        "id": "hefs7jJDZ6Ga"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n",
        "\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "image_dir = os.path.join(directory_path, 'Images/')\n",
        "#because need both for training\n",
        "training_image_paths = [image_dir + name + '.jpg' for name in (training_file_names+ validation_file_names)]\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(training_image_paths))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "metadata": {
        "id": "pFzHUcvWZEzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d588cb-82e7-45e7-8144-6e52ce186d91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.10s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "# Clean the captions data\n",
        "#    Convert all words to lowercase.\n",
        "#    Remove all punctuation.\n",
        "#    Remove all words that are one character or less in length (e.g. ‘a’).\n",
        "#    Remove all words with numbers in them.\n",
        "def captions_clean (image_dict):\n",
        "  for key, captions in image_dict.items():\n",
        "    for i, caption in enumerate (captions):\n",
        "      # Convert the caption to lowercase, and then removing all special characters from it\n",
        "      caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
        "      # Split the caption into separate words, and collect all words which are more than\n",
        "      # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
        "      clean_words = [word for word in caption_nopunct.split() if ((len(word) > 1) and (word.isalpha()))]\n",
        "      # Join those words into a string\n",
        "      caption_new = ' '.join(clean_words)\n",
        "      # Replace the old caption in the captions list with this new cleaned caption\n",
        "      captions[i] = caption_new\n",
        "\n",
        "# Add two tokens, 'startseq' and 'endseq' at the beginning and end respectively, of every caption\n",
        "def add_token (captions):\n",
        "  for i, caption in enumerate (captions):\n",
        "    captions[i] = 'startseq ' + caption + ' endseq'\n",
        "  return (captions)\n",
        "\n",
        "# Given a set of training, validation or testing image names, return a dictionary\n",
        "# containing the corresponding subset from the full dictionary of images with captions\n",
        "# This returned subset has the same structure as the full dictionary\n",
        "def subset_data_dict (image_dict, image_names):\n",
        "  dict = { image_name:add_token(captions) for image_name,captions in image_dict.items() if image_name in image_names}\n",
        "  return (dict)\n",
        "\n",
        "def subset_data_dict_notoken (image_dict, image_names):\n",
        "  dict = { image_name:captions for image_name,captions in image_dict.items() if image_name in image_names}\n",
        "  return (dict)\n",
        "\n",
        "# Flat list of all captions\n",
        "def all_captions (data_dict):\n",
        "  return ([caption for key, captions in data_dict.items() for caption in captions])\n",
        "\n",
        "# Calculate the word-length of the caption with the most words\n",
        "def max_caption_length(captions):\n",
        "  return max(len(caption.split()) for caption in captions)\n",
        "\n",
        "# Fitting a Keras tokenizer given caption descriptions\n",
        "# The tokenizer uses the captions to learn a mapping from words to numeric word indices\n",
        "def create_tokenizer(data_dict):\n",
        "  captions = all_captions(data_dict)\n",
        "  max_caption_words = max_caption_length(captions)\n",
        "  # Initialise a Keras Tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  # Fit it on the captions so that it prepares a vocabulary of all words\n",
        "  tokenizer.fit_on_texts(captions)\n",
        "  # Get the size of the vocabulary\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  return (tokenizer, vocab_size, max_caption_words)\n",
        "\n",
        "print(caption_dict)\n",
        "captions_clean(caption_dict)\n",
        "print(caption_dict)\n",
        "training_dict = subset_data_dict (caption_dict, training_file_names)\n",
        "testing_dict = subset_data_dict_notoken (caption_dict, validation_file_names)\n",
        "print(caption_dict)\n",
        "\n",
        "# Prepare tokenizer\n",
        "tokenizer, vocab_size, max_caption_words = create_tokenizer(training_dict)\n",
        "print(vocab_size, max_caption_words)\n",
        "print(training_dict)\n",
        "print(testing_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFi3YG3Hkykf",
        "outputId": "9f3566d3-1460-489a-aeb7-c2922f21b0ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1000268201_693b08cb0e': ['A little girl in a pink dress going into a wooden cabin .'], '1001773457_577c3a7d70': ['Two dogs on pavement moving toward each other .'], '1002674143_1b742ab4b8': ['Young girl with pigtails painting outside in the grass .'], '1003163366_44323f5815': ['man laying on bench holding leash of dog sitting on ground'], '1007129816_e794419615': ['The man with pierced ears is wearing glasses and an orange hat .'], '1007320043_627395c3d8': ['The small child climbs on a red ropes on a playground .'], '1009434119_febe49276a': ['A dog runs on the green grass near a wooden fence .'], '1012212859_01547e3f17': ['White dog with brown ears standing near water with head turned to one side .'], '1015118661_980735411b': ['Smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him .'], '1015584366_dfcec3c85a': ['The black dog jumped the tree stump .'], '101654506_8eb26cfb60': ['The white and brown dog is running over the surface of the snow .'], '101669240_b2d3e7f17b': ['Man on skis looking at artwork for sale in the snow'], '1016887272_03199f49c4': ['Several climbers in a row are climbing the rock while the man in red watches and holds the line .'], '1019077836_6fc9b15408': ['Large brown dog running away from the sprinkler in the grass .'], '1019604187_d087bf9a5f': ['A white dog running after a yellow ball'], '1020651753_06077ec457': ['The white dog is playing in a green field with a yellow toy .'], '1022454332_6af2c1449a': ['\"Two people are at the edge of a lake , facing the water and the city skyline .\"'], '1022454428_b6b660a67b': ['Couple with a baby sit outdoors next to their stroller .'], '1022975728_75515238d8': ['This is a black dog splashing in the water .'], '102351840_323e3de834': ['Two men are ice fishing .']}\n",
            "{'1000268201_693b08cb0e': ['little girl in pink dress going into wooden cabin'], '1001773457_577c3a7d70': ['two dogs on pavement moving toward each other'], '1002674143_1b742ab4b8': ['young girl with pigtails painting outside in the grass'], '1003163366_44323f5815': ['man laying on bench holding leash of dog sitting on ground'], '1007129816_e794419615': ['the man with pierced ears is wearing glasses and an orange hat'], '1007320043_627395c3d8': ['the small child climbs on red ropes on playground'], '1009434119_febe49276a': ['dog runs on the green grass near wooden fence'], '1012212859_01547e3f17': ['white dog with brown ears standing near water with head turned to one side'], '1015118661_980735411b': ['smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him'], '1015584366_dfcec3c85a': ['the black dog jumped the tree stump'], '101654506_8eb26cfb60': ['the white and brown dog is running over the surface of the snow'], '101669240_b2d3e7f17b': ['man on skis looking at artwork for sale in the snow'], '1016887272_03199f49c4': ['several climbers in row are climbing the rock while the man in red watches and holds the line'], '1019077836_6fc9b15408': ['large brown dog running away from the sprinkler in the grass'], '1019604187_d087bf9a5f': ['white dog running after yellow ball'], '1020651753_06077ec457': ['the white dog is playing in green field with yellow toy'], '1022454332_6af2c1449a': ['two people are at the edge of lake facing the water and the city skyline'], '1022454428_b6b660a67b': ['couple with baby sit outdoors next to their stroller'], '1022975728_75515238d8': ['this is black dog splashing in the water'], '102351840_323e3de834': ['two men are ice fishing']}\n",
            "{'1000268201_693b08cb0e': ['startseq little girl in pink dress going into wooden cabin endseq'], '1001773457_577c3a7d70': ['startseq two dogs on pavement moving toward each other endseq'], '1002674143_1b742ab4b8': ['startseq young girl with pigtails painting outside in the grass endseq'], '1003163366_44323f5815': ['startseq man laying on bench holding leash of dog sitting on ground endseq'], '1007129816_e794419615': ['startseq the man with pierced ears is wearing glasses and an orange hat endseq'], '1007320043_627395c3d8': ['startseq the small child climbs on red ropes on playground endseq'], '1009434119_febe49276a': ['startseq dog runs on the green grass near wooden fence endseq'], '1012212859_01547e3f17': ['startseq white dog with brown ears standing near water with head turned to one side endseq'], '1015118661_980735411b': ['startseq smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him endseq'], '1015584366_dfcec3c85a': ['the black dog jumped the tree stump'], '101654506_8eb26cfb60': ['startseq the white and brown dog is running over the surface of the snow endseq'], '101669240_b2d3e7f17b': ['startseq man on skis looking at artwork for sale in the snow endseq'], '1016887272_03199f49c4': ['startseq several climbers in row are climbing the rock while the man in red watches and holds the line endseq'], '1019077836_6fc9b15408': ['startseq large brown dog running away from the sprinkler in the grass endseq'], '1019604187_d087bf9a5f': ['startseq white dog running after yellow ball endseq'], '1020651753_06077ec457': ['startseq the white dog is playing in green field with yellow toy endseq'], '1022454332_6af2c1449a': ['startseq two people are at the edge of lake facing the water and the city skyline endseq'], '1022454428_b6b660a67b': ['startseq couple with baby sit outdoors next to their stroller endseq'], '1022975728_75515238d8': ['startseq this is black dog splashing in the water endseq'], '102351840_323e3de834': ['startseq two men are ice fishing endseq']}\n",
            "123 21\n",
            "{'1000268201_693b08cb0e': ['startseq little girl in pink dress going into wooden cabin endseq'], '1001773457_577c3a7d70': ['startseq two dogs on pavement moving toward each other endseq'], '1002674143_1b742ab4b8': ['startseq young girl with pigtails painting outside in the grass endseq'], '1003163366_44323f5815': ['startseq man laying on bench holding leash of dog sitting on ground endseq'], '1007129816_e794419615': ['startseq the man with pierced ears is wearing glasses and an orange hat endseq'], '1007320043_627395c3d8': ['startseq the small child climbs on red ropes on playground endseq'], '1009434119_febe49276a': ['startseq dog runs on the green grass near wooden fence endseq'], '1012212859_01547e3f17': ['startseq white dog with brown ears standing near water with head turned to one side endseq'], '1015118661_980735411b': ['startseq smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him endseq'], '101654506_8eb26cfb60': ['startseq the white and brown dog is running over the surface of the snow endseq'], '101669240_b2d3e7f17b': ['startseq man on skis looking at artwork for sale in the snow endseq'], '1016887272_03199f49c4': ['startseq several climbers in row are climbing the rock while the man in red watches and holds the line endseq'], '1019077836_6fc9b15408': ['startseq large brown dog running away from the sprinkler in the grass endseq'], '1019604187_d087bf9a5f': ['startseq white dog running after yellow ball endseq'], '1020651753_06077ec457': ['startseq the white dog is playing in green field with yellow toy endseq'], '1022454332_6af2c1449a': ['startseq two people are at the edge of lake facing the water and the city skyline endseq'], '1022454428_b6b660a67b': ['startseq couple with baby sit outdoors next to their stroller endseq'], '1022975728_75515238d8': ['startseq this is black dog splashing in the water endseq'], '102351840_323e3de834': ['startseq two men are ice fishing endseq']}\n",
            "{'1001773457_577c3a7d70': ['startseq two dogs on pavement moving toward each other endseq'], '1002674143_1b742ab4b8': ['startseq young girl with pigtails painting outside in the grass endseq'], '1003163366_44323f5815': ['startseq man laying on bench holding leash of dog sitting on ground endseq'], '1007320043_627395c3d8': ['startseq the small child climbs on red ropes on playground endseq'], '1012212859_01547e3f17': ['startseq white dog with brown ears standing near water with head turned to one side endseq'], '1015584366_dfcec3c85a': ['the black dog jumped the tree stump'], '101654506_8eb26cfb60': ['startseq the white and brown dog is running over the surface of the snow endseq'], '101669240_b2d3e7f17b': ['startseq man on skis looking at artwork for sale in the snow endseq'], '1016887272_03199f49c4': ['startseq several climbers in row are climbing the rock while the man in red watches and holds the line endseq'], '1019077836_6fc9b15408': ['startseq large brown dog running away from the sprinkler in the grass endseq'], '1020651753_06077ec457': ['startseq the white dog is playing in green field with yellow toy endseq'], '1022454332_6af2c1449a': ['startseq two people are at the edge of lake facing the water and the city skyline endseq'], '1022454428_b6b660a67b': ['startseq couple with baby sit outdoors next to their stroller endseq'], '102351840_323e3de834': ['startseq two men are ice fishing endseq']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example tokens:\")\n",
        "for word, index in list(tokenizer.word_index.items())[:10]:\n",
        "    print(f\"{word}: {index}\")\n",
        "example_caption = next(iter(training_dict.values()))[0]  # Get the first caption from the training set\n",
        "encoded_example_caption = tokenizer.texts_to_sequences([example_caption])[0]\n",
        "print(\"Example caption:\", example_caption)\n",
        "print(\"Encoded example caption:\", encoded_example_caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehc_c_KklLKq",
        "outputId": "c9d0bc16-ac10-457d-bc63-b52797c1c0a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example tokens:\n",
            "startseq: 1\n",
            "endseq: 2\n",
            "the: 3\n",
            "in: 4\n",
            "dog: 5\n",
            "on: 6\n",
            "with: 7\n",
            "man: 8\n",
            "and: 9\n",
            "white: 10\n",
            "Example caption: startseq little girl in pink dress going into wooden cabin endseq\n",
            "Encoded example caption: [1, 30, 19, 4, 31, 32, 33, 34, 20, 35, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend a list of text indices to a given fixed length\n",
        "def pad_text (text, max_length):\n",
        "  text = pad_sequences([text], maxlen=max_length, padding='post')[0]\n",
        "  return (text)\n",
        "\n",
        "def data_prep(data_dict, tokenizer, max_length, vocab_size):\n",
        "  X, y = list(), list()\n",
        "\n",
        "  # For each image and list of captions\n",
        "  for image_name, captions in data_dict.items():\n",
        "    image_name = image_dir + image_name + '.jpg'\n",
        "    # For each caption in the list of captions\n",
        "    for caption in captions:\n",
        "      # Convert the caption words into a list of word indices\n",
        "      word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
        "      # Pad the input text to the same fixed length\n",
        "      pad_idxs = pad_text(word_idxs, max_length)\n",
        "      X.append(image_name)\n",
        "      y.append(pad_idxs)\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "train_X, train_y = data_prep(training_dict, tokenizer, max_caption_words, vocab_size)\n",
        "print(train_X, train_y)\n",
        "test_X, test_y = data_prep(testing_dict, tokenizer, max_caption_words, vocab_size)\n",
        "print(test_X, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8G64FS_pTdw",
        "outputId": "933076ce-4c75-4b7f-e370-a3a1b8c647ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/My Drive/image_captions/Images/1000268201_693b08cb0e.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1001773457_577c3a7d70.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1002674143_1b742ab4b8.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1003163366_44323f5815.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1007129816_e794419615.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1007320043_627395c3d8.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1009434119_febe49276a.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1012212859_01547e3f17.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1015118661_980735411b.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/101654506_8eb26cfb60.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/101669240_b2d3e7f17b.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1016887272_03199f49c4.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1019077836_6fc9b15408.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1019604187_d087bf9a5f.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1020651753_06077ec457.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1022454332_6af2c1449a.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1022454428_b6b660a67b.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1022975728_75515238d8.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/102351840_323e3de834.jpg'] [[  1  30  19   4  31  32  33  34  20  35   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  13  36   6  37  38  39  40  41   2   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  42  19   7  43  44  45   4   3  14   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   8  46   6  47  48  49  11   5  50   6  51   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3   8   7  52  21  12  53  54   9  55  56  57   2   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3  58  59  60   6  22  61   6  62   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   5  63   6   3  23  14  24  20  64   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  10   5   7  15  21  65  24  16   7  66  67  25  68  69   2   0   0\n",
            "    0   0   0]\n",
            " [  1  70  71   4  10  72   9  73  74   4  75  11  26  76   7   8   4  77\n",
            "   78  79   2]\n",
            " [  1   3  10   9  15   5  12  17  80   3  81  11   3  27   2   0   0   0\n",
            "    0   0   0]\n",
            " [  1   8   6  82  83  28  84  85  86   4   3  27   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  87  88   4  89  18  90   3  26  91   3   8   4  22  92   9  93   3\n",
            "   94   2   0]\n",
            " [  1  95  15   5  17  96  97   3  98   4   3  14   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  10   5  17  99  29 100   2   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3  10   5  12 101   4  23 102   7  29 103   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  13 104  18  28   3 105  11 106 107   3  16   9   3 108 109   2   0\n",
            "    0   0   0]\n",
            " [  1 110   7 111 112 113 114  25 115 116   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1 117  12 118   5 119   4   3  16   2   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  13 120  18 121 122   2   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]]\n",
            "['/content/drive/My Drive/image_captions/Images/1001773457_577c3a7d70.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1002674143_1b742ab4b8.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1003163366_44323f5815.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1007320043_627395c3d8.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1012212859_01547e3f17.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1015584366_dfcec3c85a.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/101654506_8eb26cfb60.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/101669240_b2d3e7f17b.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1016887272_03199f49c4.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1019077836_6fc9b15408.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1020651753_06077ec457.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1022454332_6af2c1449a.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/1022454428_b6b660a67b.jpg'\n",
            " '/content/drive/My Drive/image_captions/Images/102351840_323e3de834.jpg'] [[  1  13  36   6  37  38  39  40  41   2   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  42  19   7  43  44  45   4   3  14   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   8  46   6  47  48  49  11   5  50   6  51   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3  58  59  60   6  22  61   6  62   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  10   5   7  15  21  65  24  16   7  66  67  25  68  69   2   0   0\n",
            "    0   0   0]\n",
            " [  3 118   5   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3  10   9  15   5  12  17  80   3  81  11   3  27   2   0   0   0\n",
            "    0   0   0]\n",
            " [  1   8   6  82  83  28  84  85  86   4   3  27   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  87  88   4  89  18  90   3  26  91   3   8   4  22  92   9  93   3\n",
            "   94   2   0]\n",
            " [  1  95  15   5  17  96  97   3  98   4   3  14   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1   3  10   5  12 101   4  23 102   7  29 103   2   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  13 104  18  28   3 105  11 106 107   3  16   9   3 108 109   2   0\n",
            "    0   0   0]\n",
            " [  1 110   7 111 112 113 114  25 115 116   2   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [  1  13 120  18 121 122   2   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BATCH_SIZE = 64\n",
        "BATCH_SIZE = 10\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "   img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "   return img_tensor, cap\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
        "# Use map to load the numpy files in parallel\n",
        "test_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# Shuffle and batch\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "VcRcP8O2w20t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "61N8XYXRx7mL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = vocab_size\n",
        "num_steps = len(train_X) // BATCH_SIZE\n",
        "num_val_steps = len(test_X) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "loss_plot = []\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "  return loss, total_loss\n",
        "\n",
        "@tf.function\n",
        "def validation_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n",
        "    features = encoder(img_tensor)\n",
        "    for i in range(1, target.shape[1]):\n",
        "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "        loss += loss_function(target[:, i], predictions)\n",
        "        dec_input = tf.expand_dims(target[:, i], 1)\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    return loss, total_loss\n",
        "\n",
        "import time\n",
        "start_epoch = 0\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "        if batch % 50 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    val_total_loss = 0\n",
        "    for (batch, (val_img_tensor, val_target)) in enumerate(test_dataset):\n",
        "        val_batch_loss, val_t_loss = validation_step(val_img_tensor, val_target)\n",
        "        val_total_loss += val_t_loss\n",
        "    val_loss = val_total_loss / num_val_steps\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec')\n",
        "    print(f'Epoch {epoch+1} Validation Loss {val_loss:.6f}\\n')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqqXhpgXynZl",
        "outputId": "ec2b3048-0be7-4956-a656-434a7790ad45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.8128\n",
            "Epoch 1 Loss 5.395481\n",
            "Time taken for 1 epoch 98.49 sec\n",
            "Epoch 1 Validation Loss 5.266945\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.3928\n",
            "Epoch 2 Loss 5.210311\n",
            "Time taken for 1 epoch 2.26 sec\n",
            "Epoch 2 Validation Loss 5.429208\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.5126\n",
            "Epoch 3 Loss 5.089406\n",
            "Time taken for 1 epoch 1.55 sec\n",
            "Epoch 3 Validation Loss 4.909826\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.4645\n",
            "Epoch 4 Loss 5.017128\n",
            "Time taken for 1 epoch 1.54 sec\n",
            "Epoch 4 Validation Loss 4.518634\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2680\n",
            "Epoch 5 Loss 4.877752\n",
            "Time taken for 1 epoch 1.42 sec\n",
            "Epoch 5 Validation Loss 4.473413\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.4586\n",
            "Epoch 6 Loss 4.658760\n",
            "Time taken for 1 epoch 1.32 sec\n",
            "Epoch 6 Validation Loss 4.198400\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.2107\n",
            "Epoch 7 Loss 4.598165\n",
            "Time taken for 1 epoch 1.38 sec\n",
            "Epoch 7 Validation Loss 3.970387\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.1075\n",
            "Epoch 8 Loss 4.414136\n",
            "Time taken for 1 epoch 1.63 sec\n",
            "Epoch 8 Validation Loss 3.851125\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.0547\n",
            "Epoch 9 Loss 4.174045\n",
            "Time taken for 1 epoch 1.88 sec\n",
            "Epoch 9 Validation Loss 4.065903\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.9591\n",
            "Epoch 10 Loss 3.902042\n",
            "Time taken for 1 epoch 2.36 sec\n",
            "Epoch 10 Validation Loss 3.229636\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.9458\n",
            "Epoch 11 Loss 3.609033\n",
            "Time taken for 1 epoch 1.93 sec\n",
            "Epoch 11 Validation Loss 3.157266\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.7418\n",
            "Epoch 12 Loss 3.355672\n",
            "Time taken for 1 epoch 1.34 sec\n",
            "Epoch 12 Validation Loss 3.013146\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.7791\n",
            "Epoch 13 Loss 3.061636\n",
            "Time taken for 1 epoch 1.62 sec\n",
            "Epoch 13 Validation Loss 2.627760\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.4017\n",
            "Epoch 14 Loss 2.839962\n",
            "Time taken for 1 epoch 1.56 sec\n",
            "Epoch 14 Validation Loss 2.632348\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.5040\n",
            "Epoch 15 Loss 2.646398\n",
            "Time taken for 1 epoch 1.59 sec\n",
            "Epoch 15 Validation Loss 2.543863\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.0525\n",
            "Epoch 16 Loss 2.518012\n",
            "Time taken for 1 epoch 1.49 sec\n",
            "Epoch 16 Validation Loss 2.338040\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.4058\n",
            "Epoch 17 Loss 2.384484\n",
            "Time taken for 1 epoch 1.47 sec\n",
            "Epoch 17 Validation Loss 2.154641\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.0301\n",
            "Epoch 18 Loss 2.301097\n",
            "Time taken for 1 epoch 2.05 sec\n",
            "Epoch 18 Validation Loss 2.055532\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.8563\n",
            "Epoch 19 Loss 2.167870\n",
            "Time taken for 1 epoch 2.28 sec\n",
            "Epoch 19 Validation Loss 2.090817\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.9280\n",
            "Epoch 20 Loss 2.001271\n",
            "Time taken for 1 epoch 2.07 sec\n",
            "Epoch 20 Validation Loss 1.889617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=smoothie)\n",
        "\n",
        "def evaluate(image, max_length):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],-1,img_tensor_val.shape[3]))\n",
        "    features = encoder(img_tensor_val)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']], 0)\n",
        "    result = []\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,features,hidden)\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        if (predicted_id == 0):\n",
        "            return result, attention_plot\n",
        "        if (tokenizer.index_word[predicted_id] == 'endseq'):\n",
        "            return result, attention_plot\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "def check_test(test_image_names, image_dict, image_dir, max_caption_words):\n",
        "  bleu_scores = []\n",
        "  for rid in range(0, len(test_image_names)):\n",
        "    image_name = test_image_names[rid]\n",
        "    real_caption = ' '.join([caption.replace('startseq ', '').replace(' endseq', '') for caption in image_dict[image_name]])\n",
        "    image_path = image_dir + image_name + '.jpg'\n",
        "    result, attention_plot = evaluate(image_path, max_caption_words)\n",
        "    #from IPython.display import Image, display\n",
        "    #display(Image(image_path))\n",
        "    print('Image Name: ', image_name)\n",
        "    print('Real Caption:', real_caption)\n",
        "    print('Prediction Caption:', ' '.join(result))\n",
        "    bleu = calculate_bleu(real_caption, ' '.join(result))\n",
        "    bleu_scores.append(bleu)\n",
        "    print()\n",
        "  return bleu_scores\n",
        "\n",
        "bleu_scores = check_test(testing_file_names, caption_dict, image_dir, max_caption_words)\n",
        "print(bleu_scores)\n",
        "print(f'Average BLEU score :{sum(bleu_scores)/len(bleu_scores):.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHJfzSrYrTfu",
        "outputId": "d86c680d-0304-427b-a108-89f5819321f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Image Name:  101669240_b2d3e7f17b\n",
            "Real Caption: man on skis looking at artwork for sale in the snow\n",
            "Prediction Caption: in skis sale dog looking at looking for sale sale for skis man on looking skis artwork looking at for looking\n",
            "\n",
            "Image Name:  1015118661_980735411b\n",
            "Real Caption: smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him\n",
            "Prediction Caption: smiling behind boy in man shirt him him\n",
            "\n",
            "Image Name:  101654506_8eb26cfb60\n",
            "Real Caption: the white and brown dog is running over the surface of the snow\n",
            "Prediction Caption: the brown over brown over the white brown over brown surface running is brown brown the over and snow and the\n",
            "\n",
            "Image Name:  1015584366_dfcec3c85a\n",
            "Real Caption: the black dog jumped the tree stump\n",
            "Prediction Caption: the large\n",
            "\n",
            "Image Name:  1007320043_627395c3d8\n",
            "Real Caption: the small child climbs on red ropes on playground\n",
            "Prediction Caption: the on the toward on the on small on climbs on on red child small on red climbs on red on\n",
            "\n",
            "Image Name:  1019077836_6fc9b15408\n",
            "Real Caption: large brown dog running away from the sprinkler in the grass\n",
            "Prediction Caption: large sprinkler in dog dog sprinkler in large grass\n",
            "\n",
            "Image Name:  1020651753_06077ec457\n",
            "Real Caption: the white dog is playing in green field with yellow toy\n",
            "Prediction Caption: playing dog with yellow green field yellow rock\n",
            "\n",
            "Image Name:  1015584366_dfcec3c85a\n",
            "Real Caption: the black dog jumped the tree stump\n",
            "Prediction Caption: the next\n",
            "\n",
            "Image Name:  1022454428_b6b660a67b\n",
            "Real Caption: couple with baby sit outdoors next to their stroller\n",
            "Prediction Caption: stroller\n",
            "\n",
            "Image Name:  102351840_323e3de834\n",
            "Real Caption: two men are ice fishing\n",
            "Prediction Caption: men are ice ice two fishing\n",
            "\n",
            "Image Name:  1012212859_01547e3f17\n",
            "Real Caption: white dog with brown ears standing near water with head turned to one side\n",
            "Prediction Caption: ears ears to dog near turned side white dog one near side white dog side turned brown dog standing near water\n",
            "\n",
            "Image Name:  1015118661_980735411b\n",
            "Real Caption: smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him\n",
            "Prediction Caption: him with behind him behind him\n",
            "\n",
            "Image Name:  1007129816_e794419615\n",
            "Real Caption: the man with pierced ears is wearing glasses and an orange hat\n",
            "Prediction Caption: orange orange an and wearing and the man with ears glasses is ears pierced hat pierced pierced\n",
            "\n",
            "Image Name:  1000268201_693b08cb0e\n",
            "Real Caption: little girl in pink dress going into wooden cabin\n",
            "Prediction Caption: going girl in into girl pink wooden dress into girl\n",
            "\n",
            "Image Name:  1001773457_577c3a7d70\n",
            "Real Caption: two dogs on pavement moving toward each other\n",
            "Prediction Caption: two other dogs on two pavement on toward dogs two dogs two on two pavement toward each pavement dogs toward each\n",
            "\n",
            "Image Name:  1015118661_980735411b\n",
            "Real Caption: smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him\n",
            "Prediction Caption: him\n",
            "\n",
            "Image Name:  1001773457_577c3a7d70\n",
            "Real Caption: two dogs on pavement moving toward each other\n",
            "Prediction Caption: two two each two moving\n",
            "\n",
            "Image Name:  1015584366_dfcec3c85a\n",
            "Real Caption: the black dog jumped the tree stump\n",
            "Prediction Caption: \n",
            "\n",
            "Image Name:  1020651753_06077ec457\n",
            "Real Caption: the white dog is playing in green field with yellow toy\n",
            "Prediction Caption: playing dog running in the toy with playing yellow playing with running toy in green toy yellow sitting yellow water playing\n",
            "\n",
            "Image Name:  1016887272_03199f49c4\n",
            "Real Caption: several climbers in row are climbing the rock while the man in red watches and holds the line\n",
            "Prediction Caption: several while the the line in red red red climbing the man in row climbers in line and several line are\n",
            "\n",
            "[0.36379933804610143, 0.1985434637571699, 0.4647834977308386, 0.02185542087031417, 0.36510065544933784, 0.5005206125602092, 0.6138915489819149, 0.013091952723013044, 0.004086771438464067, 0.7459429832584497, 0.5093422877486938, 0.07039679122179121, 0.5796068829700356, 0.6693416006279931, 0.3197433629765008, 2.7659670451403658e-14, 0.23360766477326314, 0, 0.309944235213793, 0.6834757307764594]\n",
            "Average BLEU score :0.333354\n"
          ]
        }
      ]
    }
  ]
}